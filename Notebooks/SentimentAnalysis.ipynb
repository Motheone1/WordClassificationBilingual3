{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12fa5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import random\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB , GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression , SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "import pickle\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "import re\n",
    "import csv \n",
    "from datetime import datetime , timedelta\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# %%\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Required once\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c84bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuetral  = open(\"../data/neutral.txt\", 'r', encoding='utf-8').read()\n",
    "positive = open(\"../data/positive.txt\", 'r', encoding='utf-8').read()\n",
    "negative = open(\"../data/negative.txt\", 'r', encoding='utf-8').read()\n",
    "nuetralAR = open(\"../data/neutral.en.ar.txt\", 'r', encoding='utf-8').read()\n",
    "positiveAR = open(\"../data/positive.en.ar.txt\", 'r', encoding='utf-8').read()\n",
    "negativeAR = open(\"../data/negative.en.ar.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "\n",
    "nuetral = nuetral.split('\\n')\n",
    "positive = positive.split('\\n')\n",
    "negative = negative.split('\\n')\n",
    "nuetralAR = nuetralAR.split('\\n')\n",
    "positiveAR = positiveAR.split('\\n')\n",
    "negativeAR = negativeAR.split('\\n')\n",
    "stopwordsAR = open(\"../data/listofStopWords.txt\", 'r', encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "993100c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 8582\n",
      "Negative: 7781\n",
      "Neutral: 11118\n",
      "Positive AR: 8582\n",
      "Negative AR: 7781\n",
      "Neutral AR: 11118\n"
     ]
    }
   ],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "stemmer  = ArabicLightStemmer()\n",
    "\n",
    "list_of_stopwordsAr = stopwordsAR.split('\\n')\n",
    "punctuation = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~،؛؟ـ«»…'''\n",
    "\n",
    "\n",
    "# %%\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nuetral0 = [word for word in nuetral if word]\n",
    "positive0 = [word for word in positive if word] \n",
    "negative0 = [word for word in negative if word]\n",
    "nuetralAR0 = [word for word in nuetralAR if word]\n",
    "positiveAR0 = [word for word in positiveAR if word]\n",
    "negativeAR0 = [word for word in negativeAR if word]\n",
    "\n",
    "print(\"Positive:\", len(positive0))\n",
    "print(\"Negative:\", len(negative0))\n",
    "print(\"Neutral:\", len(nuetral0))\n",
    "print(\"Positive AR:\", len(positiveAR0))\n",
    "print(\"Negative AR:\", len(negativeAR0))\n",
    "print(\"Neutral AR:\", len(nuetralAR0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42be9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(sentence):\n",
    "    token = word_tokenize(sentence.lower())\n",
    "    # print(\"TokeN\", token)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in token if word.isalpha() and word not in stopwords]\n",
    "    # print(\"Tokens\", tokens)\n",
    "    sentence  = ' '.join(tokens)\n",
    "    # print(\"After\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def texter (lst):\n",
    "    new_lst = []\n",
    "    for i in range(len(lst)):\n",
    "        # print(\"before\", lst[i])\n",
    "        lst[i] = text_cleaner(lst[i])\n",
    "        if lst[i] == '' or lst[i] == ' ' or lst[i] == '  ' or lst[i] == '   ':\n",
    "            continue\n",
    "        # print(lst[i])\n",
    "        # print(\"after\", lst[i])\n",
    "        if lst[i] != '':\n",
    "            new_lst.append(lst[i])   \n",
    "    return new_lst\n",
    "\n",
    "# %%\n",
    "def Arabic_text_cleaner(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in range(len(tokens)):\n",
    "        if tokens[token] in list_of_stopwordsAr:\n",
    "            tokens[token] = ''             \n",
    "    stemmed_tokens = [stemmer.light_stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "    \n",
    "def Arabic_texter (lst):\n",
    "    new_lst = []\n",
    "    for i in range(len(lst)):\n",
    "        # print(\"before\", lst[i])\n",
    "        if lst[i] == '' or lst[i] == ' ' or lst[i] == '  ' or lst[i] == '   ' or lst[i] == '\\n' or lst[i] == '\\t':\n",
    "            continue\n",
    "        lst[i] = Arabic_text_cleaner(lst[i])\n",
    "        \n",
    "        # print(lst[i])\n",
    "        # print(\"after\", lst[i])\n",
    "        \n",
    "        new_lst.append(lst[i])   \n",
    "    return new_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3528d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuetral0 = texter(nuetral0)\n",
    "positive0 = texter(positive0)\n",
    "negative0 = texter(negative0)\n",
    "nuetralAR0 = Arabic_texter(nuetralAR0)\n",
    "positiveAR0 = Arabic_texter(positiveAR0)\n",
    "negativeAR0 = Arabic_texter(negativeAR0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3423f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for w in positive0:\n",
    "    documents.append((w, 'positive'))\n",
    "for w in negative0:\n",
    "    documents.append((w, 'negative'))\n",
    "for w in nuetral0:\n",
    "    documents.append((w, 'neutral'))\n",
    "for w in positiveAR0:\n",
    "    documents.append((w, 'positive'))\n",
    "for w in negativeAR0:\n",
    "    documents.append((w, 'negative'))\n",
    "for w in nuetralAR0:\n",
    "    documents.append((w, 'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f425a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04d34ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: [('drs office mine drive nut especially appt kid', 'negative'), (\"سم ضا ( Soulja Boy Tell 'Em LIVE live > http : //ustre.am/2UhS )\", 'neutral'), ('hungry sub', 'neutral'), ('  حب أشخاص سعداء لامع .  خبير محل  حرك عين سريعة، رأ ول   ١٩٨٦ .', 'negative'), ('وفرت', 'neutral'), (\"`` أسف  منى ذلك، عن أن   أحي قيم حفل مر  سنة،   دائما ''\", 'negative'), ('_d Zwarte maillot', 'neutral'), ('fever', 'negative'), ('قت غداء قريب', 'neutral'), ('trending say well tweetcannon http', 'neutral')]\n"
     ]
    }
   ],
   "source": [
    "print(\"documents:\", documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c3a5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []   # goes into vectorizer\n",
    "labels = []  # goes into classifier\n",
    "\n",
    "for text, label in documents:\n",
    "    texts.append(text)    # cleaned string\n",
    "    labels.append(label)  # topic/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8471167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['drs office mine drive nut especially appt kid', \"سم ضا ( Soulja Boy Tell 'Em LIVE live > http : //ustre.am/2UhS )\", 'hungry sub', '  حب أشخاص سعداء لامع .  خبير محل  حرك عين سريعة، رأ ول   ١٩٨٦ .', 'وفرت', \"`` أسف  منى ذلك، عن أن   أحي قيم حفل مر  سنة،   دائما ''\", '_d Zwarte maillot', 'fever', 'قت غداء قريب', 'trending say well tweetcannon http', 'قد قرأ  ضع سابيع، قد عمل  جيد غا  فيلم .', 'happy mother day hope tom got something special enjoy day xo', 'maybe cell phone soon yeah', 'حاول علم يف استخدام ويتر ... نجح . مرحبا جميع متابعي،  نن ست مثيرا اهتمام .', 'gutted handbag wanted sold', 'عد  مكتب قضاء سبوع مزدحم -  عطل  أسبوع  رائع', \"`` او،  قد عائل حقا  . سيء غا . ''\", 'ها ... آسفة،  جب    كل  شخص تبع', ' ريد حقا ذهاب  مدرس  اثن كو صادقا', 'hot texas ac upstairs broken really hot house', ' صحيح ماما ! عجب سلوب مك !', \"`` ريد  رتد ملابس أخرج،     ذهب معه،  مع يا مرشد ''\", 'صور  تم حديث .', '  تزوج', \"`` حا قت نوم،  منى    شخص مستلق جانب ''\", 'بي  منزل جري مؤتمرا هاتفيا . معذرة .', 'fail http work', 'game bad leaving hi', 'حب ثيرا -   سؤال  -   وجد موعد جول Full Moon Crazy  تورنت ! ؟ ! ! ؟ ! ! ؟؟ !', 'sneezing frequently', \"منى    خيار `` عجب '' (  سب ) شياء  \", 'best friend away special olympics said gon na bring', 'آمل  سمع شيئا قريبا  جراح ريستالز', 'شعر ذعر قليلا .. تمنى  سير أمور   رام', 'ذهاب  محكم دفع ثم علام ضرائب   سيارا ...  كو مكلفا .', 'blue sky still grey hazy window', ' قصد  ملف شخص سيء؟  حالف حظ  . تقدمت حوال ٣٠ ظيف   .', 'nw macedonia rainy day row', 'believe finish tonight blame errand certain loud bookstore', 'graduation done im little sad anyone want hang', \"``  ! خيرا جاء  جمعة، أنا عالق  مشاريع ''\", 'ود  .  دي عطل  سبوع مجان  مرة، عائل زور . ذا بقى  .  مر قادم .', 'هس ...', 'http : //twitpic.com/4fnaa - ها حب يا تا', 'stumble fall moved', 'agreed', 'مرحبا عالم صغير !  حالك؟', 'slept missed bus train delayed stop stop late work heelllll', \"`` عتقد  ارتداء برمود  سينم  كن رة جيد   شيء،  جو  خارج  رطبا غا ! ''\", 'okay bed ummm ummm talk need talk somethings good night tweeter', 'lololol love kenan kel rock soo hard im watching right', 'ملح خل', 'starting new diet today want get fat besides almost bathing suit season lol', '  ريقيا،     ضا هناك عاصف بير ', 'اج صعوب  استخدام Hulu،  سمع   شخص عا  مشاكل  استخدام Windows Vista؟', 'Awww that ` s so sweet Wish you could see yourz 2', 'عمل  منزل  متابع  شيء استثناء ويتر', \"رأيت تو  أمه عازب بدين  على طريق،  تمدد `` شكل مغر ''  غطاء محر سيار رجل قير . ربم حتاج  صدمات جديد .\", \"`` عيد  سعيد مي ( يضا ختي مي جيس، هه ) ''\", 'looool chip ketchup ya waili let fish finger sure baked bean though', 'hav fun heav metal happy hour guy future accadentally set fire smoking', 'ستار ري ..  خيب ظن ! ٥ جوم ! !', 'lol marketing', 'بدو أن  عد  فاق ... حز', 'sad face moment day', '   ستطيع  حمل  .', 'stop mean ur hurting feeling', 'hi everybody sorry long listening iwas busy', '  حدث  همل حديق طوال ربيع ! شكرا ك، تمنى رؤ لابك !', 'yeah twitter lot fun babe especially got ppl talk lol sound goooooood lol', ' تظاهر ذلك ! هاه ... مزح .', \"`` سعد سماع ذلك، كن  زال شعر خجل حيانا  سمع  فعل  رجال ''\", 'wait see yall friday hope meet u guy would make night even better', 'صنع  شيئا . شراء شيء  مر  شخص .  مك !', \"`` عيشوا،   ريد . كلما زاد  ضل .  مؤكد   بيض اسد، لكن تعلم قريبا . ''\", 'happy mother day wonderful mom mom make world stay balanced great mother day', 'startin get head ache uuuggghh', 'hoping fixed right', 'miss something', 'قد تلقيت هداي عيد ميلاد رائع !  حفل ناجحا عاد .', 'love johnny deep look public enemy l catch film show hk', 'wong', \"``  لست سعيدا جدا، شاهد قنا ديزن ... تاف تاف تافه،  حصل  ملصق جوناس ثلاث أبعاد خاص ي، رجل  كن  كن ... ''\", ' وقظ ثيرا  طريق  سا فرانسيسكو، صبح  خير مريك', 'عيد أم   أحد قادم ....  نسى  رسل  شيئا مميزا http : //www.youtube.com/watch ? v=ExpmTEVSFQg', 'around little earlier wanted phone rang exercise good way start day right', 'نتهى خيرا  طلاء حمام . آن، حا قت لمع .', \"`` حا قت مدرس يا رفاق !  حرب نجوم سعيد ''\", 'glad u feel like cooking tired', 'star trek great yet minor detail needed worked give thumb regardless', 'hahaha really trying figure last time fb looking friend page', ' تجاهل', 'hey split rock know llama anywhere', 'قد خبر حد ذلك فعل ! ! ول', 'قد سعت  قراص اذع  طول ساق', ' معدل ذكائ مرتفع  تعلق أمر معرف  يف سباح  دائر  وعاء ...', 'مراقب جير .', 'life good', 'actually love nesquik cereal', 'شكرا !  واقع،  حصل  حساب  ويتر ول مرة،  كن صور حمل ضا . لكن تحمل، أمر يستغرق قتا .']\n",
      "Labels: ['negative', 'neutral', 'neutral', 'negative', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(\"Texts:\", texts[:100])  # Print first 5 entries to verify\n",
    "print(\"Labels:\", labels[:5])  # Print first 5 entries to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc0b961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer ,TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)  # only the texts are transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d191a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.10, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07531c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.17353262850894 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.57      0.64      1610\n",
      "     neutral       0.61      0.76      0.68      2211\n",
      "    positive       0.77      0.69      0.73      1665\n",
      "\n",
      "    accuracy                           0.68      5486\n",
      "   macro avg       0.70      0.67      0.68      5486\n",
      "weighted avg       0.69      0.68      0.68      5486\n",
      "\n",
      "\n",
      "Top words for class 'negative':\n",
      "   sad  (weight: 5.5076)\n",
      "   hate  (weight: 4.7835)\n",
      "   miss  (weight: 4.6771)\n",
      "   sorry  (weight: 4.6136)\n",
      "   suck  (weight: 4.4654)\n",
      "   حز  (weight: 4.2479)\n",
      "   آسف  (weight: 4.2212)\n",
      "   فتقد  (weight: 3.8516)\n",
      "   stupid  (weight: 3.8133)\n",
      "   سيئ  (weight: 3.7485)\n",
      "   ؤلم  (weight: 3.7096)\n",
      "   sick  (weight: 3.7003)\n",
      "   bored  (weight: 3.6676)\n",
      "   poor  (weight: 3.6078)\n",
      "   سيء  (weight: 3.4443)\n",
      "   tired  (weight: 3.4209)\n",
      "   missing  (weight: 3.4178)\n",
      "   غب  (weight: 3.3674)\n",
      "   مريض  (weight: 3.3035)\n",
      "   hurt  (weight: 3.2763)\n",
      "\n",
      "Top words for class 'neutral':\n",
      "   لكن  (weight: 1.4134)\n",
      "   ذاهب  (weight: 1.3321)\n",
      "   starwarsday  (weight: 1.2782)\n",
      "   indoors  (weight: 1.1958)\n",
      "   فور  (weight: 1.1443)\n",
      "   tour  (weight: 1.1417)\n",
      "   مغامر  (weight: 1.1216)\n",
      "   training  (weight: 1.1123)\n",
      "   resist  (weight: 1.0679)\n",
      "   otherwise  (weight: 1.0667)\n",
      "   مختلف  (weight: 1.0610)\n",
      "   تسكع  (weight: 1.0367)\n",
      "   اعتقد  (weight: 1.0359)\n",
      "   أزرق  (weight: 1.0294)\n",
      "   جنب  (weight: 1.0254)\n",
      "   gots  (weight: 1.0009)\n",
      "   نادي  (weight: 0.9999)\n",
      "   لاحظ  (weight: 0.9937)\n",
      "   except  (weight: 0.9923)\n",
      "   bound  (weight: 0.9920)\n",
      "\n",
      "Top words for class 'positive':\n",
      "   رائع  (weight: 6.4677)\n",
      "   love  (weight: 6.2140)\n",
      "   شكرا  (weight: 5.9402)\n",
      "   thanks  (weight: 5.7189)\n",
      "   awesome  (weight: 5.4532)\n",
      "   سعيد  (weight: 5.3356)\n",
      "   حب  (weight: 5.0278)\n",
      "   great  (weight: 4.8440)\n",
      "   جميل  (weight: 4.7972)\n",
      "   شكر  (weight: 4.6899)\n",
      "   happy  (weight: 4.6626)\n",
      "   thank  (weight: 4.6157)\n",
      "   nice  (weight: 4.5402)\n",
      "   good  (weight: 4.2798)\n",
      "   amazing  (weight: 4.1256)\n",
      "   ستمتع  (weight: 4.0276)\n",
      "   طيف  (weight: 3.8750)\n",
      "   رائعا  (weight: 3.8709)\n",
      "   excited  (weight: 3.8186)\n",
      "   hope  (weight: 3.7773)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# %%\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "num_top_words = 20  # You can change this\n",
    "\n",
    "for i, class_label in enumerate(model.classes_):\n",
    "    top_indices = np.argsort(model.coef_[i])[-num_top_words:]\n",
    "    print(f\"\\nTop words for class '{class_label}':\")\n",
    "    for index in reversed(top_indices):\n",
    "        print(f\"   {feature_names[index]}  (weight: {model.coef_[i][index]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe82ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(text, vectorizer, model):\n",
    "    if bool(re.search(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]', text)):\n",
    "        print(\"Arabic text detected, using Arabic text cleaner.\")\n",
    "        print(\"the text is:\", text )\n",
    "        cleaned_text = Arabic_text_cleaner(text)\n",
    "    else:\n",
    "        print(\"English text detected, using English text cleaner.\")\n",
    "        cleaned_text = text_cleaner(text)\n",
    "        print(\"the text is:\", text )\n",
    "    cleaned_text = [cleaned_text]  # Ensure cleaned_text is a list for vectorization\n",
    "     # Wrap in a list for vectorization\n",
    "    print(\"Cleaned Text:\", cleaned_text)  # Print cleaned text for debugging\n",
    "    tfidf_vector = vectorizer.transform(cleaned_text)\n",
    "    prediction = model.predict(tfidf_vector)\n",
    "\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "766046fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text detected, using English text cleaner.\n",
      "the text is: I want to know about the leaves policy\n",
      "Cleaned Text: ['want know leaf policy']\n",
      "Input: I want to know about the leaves policy\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: What are the benefits of working here?\n",
      "Cleaned Text: ['benefit working']\n",
      "Input: What are the benefits of working here?\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: What are the career and learning opportunities?\n",
      "Cleaned Text: ['career learning opportunity']\n",
      "Input: What are the career and learning opportunities?\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: What facilities are available for employees?\n",
      "Cleaned Text: ['facility available employee']\n",
      "Input: What facilities are available for employees?\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: What are the general policies of the company?\n",
      "Cleaned Text: ['general policy company']\n",
      "Input: What are the general policies of the company?\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: how can someone proceed with a promotion?\n",
      "Cleaned Text: ['someone proceed promotion']\n",
      "Input: how can someone proceed with a promotion?\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: كيف يمكنني التقديم على الترقيات؟\n",
      "Cleaned Text: [' مك تقديم  ترقيات؟']\n",
      "Input: كيف يمكنني التقديم على الترقيات؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي سياسة الإجازات؟\n",
      "Cleaned Text: ['  سياس إجازات؟']\n",
      "Input: ما هي سياسة الإجازات؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي المزايا التي تقدمها الشركة؟\n",
      "Cleaned Text: ['  مزاي  قدم شركة؟']\n",
      "Input: ما هي المزايا التي تقدمها الشركة؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي الفرص المتاحة للتعلم والتطوير؟\n",
      "Cleaned Text: ['  فرص متاح تعلم تطوير؟']\n",
      "Input: ما هي الفرص المتاحة للتعلم والتطوير؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي المرافق المتاحة للموظفين؟\n",
      "Cleaned Text: ['  مرافق متاح موظفين؟']\n",
      "Input: ما هي المرافق المتاحة للموظفين؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي السياسات العامة للشركة؟ما هي إجراءات الترقية؟\n",
      "Cleaned Text: ['  سياس عام شركة؟م  إجراء ترقية؟']\n",
      "Input: ما هي السياسات العامة للشركة؟ما هي إجراءات الترقية؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: كيف يمكنني التقديم على الترقيات؟\n",
      "Cleaned Text: [' مك تقديم  ترقيات؟']\n",
      "Input: كيف يمكنني التقديم على الترقيات؟\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ما هي سياسة الإجازات؟\n",
      "Cleaned Text: ['  سياس إجازات؟']\n",
      "Input: ما هي سياسة الإجازات؟\n",
      "Predicted Topic: neutral\n",
      "English text detected, using English text cleaner.\n",
      "the text is: i am excited to work here\n",
      "Cleaned Text: ['excited work']\n",
      "Input: i am excited to work here\n",
      "Predicted Topic: positive\n",
      "English text detected, using English text cleaner.\n",
      "the text is: I am very happy with the work environment\n",
      "Cleaned Text: ['happy work environment']\n",
      "Input: I am very happy with the work environment\n",
      "Predicted Topic: positive\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: أنا متحمس للعمل هنا\n",
      "Cleaned Text: [' متحمس عمل ']\n",
      "Input: أنا متحمس للعمل هنا\n",
      "Predicted Topic: positive\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: أنا سعيد جدًا ببيئة العمل\n",
      "Cleaned Text: [' سعيد جدا بيئ عمل']\n",
      "Input: أنا سعيد جدًا ببيئة العمل\n",
      "Predicted Topic: positive\n",
      "English text detected, using English text cleaner.\n",
      "the text is: I am looking forward to the new project\n",
      "Cleaned Text: ['looking forward new project']\n",
      "Input: I am looking forward to the new project\n",
      "Predicted Topic: positive\n",
      "English text detected, using English text cleaner.\n",
      "the text is: i hate this job\n",
      "Cleaned Text: ['hate job']\n",
      "Input: i hate this job\n",
      "Predicted Topic: negative\n",
      "English text detected, using English text cleaner.\n",
      "the text is: I love this job\n",
      "Cleaned Text: ['love job']\n",
      "Input: I love this job\n",
      "Predicted Topic: positive\n",
      "English text detected, using English text cleaner.\n",
      "the text is: I am not happy with the work environment\n",
      "Cleaned Text: ['happy work environment']\n",
      "Input: I am not happy with the work environment\n",
      "Predicted Topic: positive\n",
      "English text detected, using English text cleaner.\n",
      "the text is: I am very satisfied with the work environment\n",
      "Cleaned Text: ['satisfied work environment']\n",
      "Input: I am very satisfied with the work environment\n",
      "Predicted Topic: neutral\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: أنا أكره هذه الوظيفة\n",
      "Cleaned Text: [' كر  وظيف']\n",
      "Input: أنا أكره هذه الوظيفة\n",
      "Predicted Topic: negative\n",
      "Arabic text detected, using Arabic text cleaner.\n",
      "the text is: ان لا اطيق هذه الوظيفة\n",
      "Cleaned Text: ['  طيق  وظيف']\n",
      "Input: ان لا اطيق هذه الوظيفة\n",
      "Predicted Topic: positive\n"
     ]
    }
   ],
   "source": [
    "listoftest = [\n",
    "    \"I want to know about the leaves policy\",  \n",
    "    \"What are the benefits of working here?\",\n",
    "    \"What are the career and learning opportunities?\",\n",
    "    \"What facilities are available for employees?\",\n",
    "    \"What are the general policies of the company?\",\n",
    "    \"how can someone proceed with a promotion?\",\n",
    "    \"كيف يمكنني التقديم على الترقيات؟\",\n",
    "    \"ما هي سياسة الإجازات؟\",\n",
    "    \"ما هي المزايا التي تقدمها الشركة؟\",\n",
    "    \"ما هي الفرص المتاحة للتعلم والتطوير؟\",\n",
    "    \"ما هي المرافق المتاحة للموظفين؟\",\n",
    "    \"ما هي السياسات العامة للشركة؟\"\n",
    "    \"ما هي إجراءات الترقية؟\",\n",
    "    \"كيف يمكنني التقديم على الترقيات؟\",\n",
    "    \"ما هي سياسة الإجازات؟\",\n",
    "    \"i am excited to work here\",\n",
    "    \"I am very happy with the work environment\",\n",
    "    \"أنا متحمس للعمل هنا\",\n",
    "    \"أنا سعيد جدًا ببيئة العمل\",\n",
    "    \"I am looking forward to the new project\",\n",
    "    \"i hate this job\",\n",
    "    \"I love this job\",\n",
    "    \"I am not happy with the work environment\",\n",
    "    \"I am very satisfied with the work environment\",\n",
    "    \"أنا أكره هذه الوظيفة\",\"ان لا اطيق هذه الوظيفة\"\n",
    "]\n",
    "for test in listoftest:\n",
    "    label = predict_topic(test, vectorizer, model)\n",
    "    print(f\"Input: {test}\")\n",
    "    print(f\"Predicted Topic: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91a855f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling :\n",
    "import pickle\n",
    "with open(\"model2.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "# Save the vectorizer\n",
    "with open(\"vectorizer2.pkl\", \"wb\") as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TopicClassifier (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
